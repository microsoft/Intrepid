{
"learning_rate": 0.00025,
"num_homing_policy": 48,
"encoder_training_num_samples": 4000,
"encoder_training_epoch": 100,
"encoder_training_lr": 0.00025,
"encoder_training_batch_size": 32,
"validation_data_percent": 0.2,
"psdp_training_num_samples": 8000,
"cb_oracle_epoch": 20,
"cb_oracle_lr": 0.00025,
"cb_oracle_batch_size": 32,
"cb_validation_pct": 0.2,
"cb_patience": 5,
"eval_homing_policy_sample_size": 50,
"n_feature_maps": 64,
"n_hidden": 512,
"p_dropout": 0.0,
"phi_layer_size": 25,
"entropy_reg_coeff": 0.075,
"bootstrap_encoder_model": false,
"failed_homing_policy_filter": false,
"encoder_sampling_style": "reuse",
"data_aggregation": false,
"reward_free_planner": "pps",
"reward_sensitive_planner": "psdp",
"patience": 30,
"bias_homing_policy": false,
"entropy_policy": "none",
"filter_unreachable_abstract_states": true,
"filter_old_abstract_states": true,
"use_l1_penalty": false,
"expected_optima": 0.0,
"max_try": 1,
"reward_type": "deterministic",
"count_type": "state"
}
